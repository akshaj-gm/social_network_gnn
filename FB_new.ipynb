{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528d806a-a8e7-4c23-9c54-8a8013406802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.1%2Bcu118.html\n",
      "Requirement already satisfied: torch-scatter in /Users/akshaj.g/mamba/lib/python3.10/site-packages (2.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-2.0.1%2Bcu118.html\n",
      "Requirement already satisfied: torch-sparse in /Users/akshaj.g/mamba/lib/python3.10/site-packages (0.6.17)\n",
      "Requirement already satisfied: scipy in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-sparse) (1.10.0)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from scipy->torch-sparse) (1.24.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch-geometric in /Users/akshaj.g/mamba/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: tqdm in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-geometric) (4.64.1)\n",
      "Requirement already satisfied: numpy in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-geometric) (1.24.1)\n",
      "Requirement already satisfied: scipy in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-geometric) (1.10.0)\n",
      "Requirement already satisfied: jinja2 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-geometric) (2.28.1)\n",
      "Requirement already satisfied: pyparsing in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-geometric) (1.2.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-geometric) (5.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from requests->torch-geometric) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from requests->torch-geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from requests->torch-geometric) (2022.12.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install torch geometric\n",
    "import os\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-2.0.1%2Bcu118.html\n",
    "  !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-2.0.1%2Bcu118.html\n",
    "  !pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbac6c36-7836-4075-9d3e-443c71863c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_geometric\n",
    "torch_geometric.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004903ff-2e54-46c8-b5f5-14c05b5de100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Union, Tuple, Optional\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\n",
    "                                    OptTensor)\n",
    "\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "\n",
    "class GNNStack(torch.nn.Module):                                                   #this makes a subclass of the nn.Module \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):        #instance of the subclass(constructor method)\n",
    "        super(GNNStack, self).__init__()                                           # initiates GNNstack instance \n",
    "        conv_model = self.build_conv_model(args.model_type)                        #building the conv layers (3 for us)         \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
    "        assert (args.num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(args.num_layers-1):\n",
    "            self.convs.append(conv_model(args.heads * hidden_dim, hidden_dim))\n",
    "\n",
    "\n",
    "        self.post_mp = nn.Sequential(                                                  #post processing dropout\n",
    "            nn.Linear(args.heads * hidden_dim, hidden_dim), nn.Dropout(args.dropout),\n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.dropout = args.dropout\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        self.emb = emb\n",
    "\n",
    "    def build_conv_model(self, model_type):\n",
    "        if model_type == 'GraphSage':\n",
    "            return GraphSage\n",
    "        elif model_type == 'GAT':\n",
    "            return GAT\n",
    "        \n",
    "    def forward(self,data):\n",
    "        x, pos_edges, neg_edges= data.x, data.edge_index , data.neg_edges\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x,pos_edges,neg_edges)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout,training=self.training)\n",
    "            \n",
    "        x=self.post_mp(x)\n",
    "        \n",
    "        if self.emb==True:\n",
    "            return x\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def get_node_embeddings(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=False)  # Set training to False to get deterministic output\n",
    "        return x\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        return F.binary_cross_entropy_with_logits(pred, label) # binary cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "108afeb8-2e5c-4c12-8600-335af0a5c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSage(MessagePassing):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, normalize = True,\n",
    "                 bias = False, **kwargs):  \n",
    "        super(GraphSage, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.lin_l = None\n",
    "        self.lin_r = None\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Your code here! \n",
    "        # Define the layers needed for the message and update functions below.\n",
    "        # self.lin_l is the linear transformation that you apply to embedding \n",
    "        #            for central node.\n",
    "        # self.lin_r is the linear transformation that you apply to aggregated \n",
    "        #            message from neighbors.\n",
    "        # Don't forget the bias!\n",
    "        # Our implementation is ~2 lines, but don't worry if you deviate from this.\n",
    "        self.lin_l = nn.Linear(self.in_channels, self.out_channels, bias=True)\n",
    "        self.lin_r = nn.Linear(self.in_channels, self.out_channels, bias=True)\n",
    "        ############################################################################\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "\n",
    "    def forward(self, x,edge_index,neg_edges,size = None):\n",
    "        out = None\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Your code here! \n",
    "        # Implement message passing, as well as any post-processing (our update rule).\n",
    "        # 1. Call the propagate function to conduct the message passing.\n",
    "        #    1.1 See the description of propagate above or the following link for more information: \n",
    "        #        https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
    "        #    1.2 We will only use the representation for neighbor nodes (x_j), so by default\n",
    "        #        we pass the same representation for central and neighbor nodes as x=(x, x). \n",
    "        # 2. Update our node embedding with skip connection from the previous layer.\n",
    "        # 3. If normalize is set, do L-2 normalization (defined in \n",
    "        #    torch.nn.functional)\n",
    "        #\n",
    "        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n",
    "        \n",
    "        pos=self.propagate(pos_edges, x=(x, x), size=size)\n",
    "        neg=self.propagate(neg_edges, x=(x, x), size=size)\n",
    "        out=self.lin_l(x) + self.lin_r(pos) - self.lin_r(neg)\n",
    "        if self.normalize:\n",
    "            out = F.normalize(out, p=2)\n",
    "        ############################################################################\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):\n",
    "\n",
    "        out = None\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Your code here! \n",
    "        # Implement your message function here.\n",
    "        # Hint: Look at the formulation of the mean aggregation function, focusing on \n",
    "        # what message each neighboring node passes.\n",
    "        #\n",
    "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
    "        out = x_j\n",
    "        ############################################################################\n",
    "\n",
    "        return out\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size = None):\n",
    "\n",
    "        out = None\n",
    "\n",
    "        # The axis along which to index number of nodes.\n",
    "        node_dim = self.node_dim\n",
    "\n",
    "        ############################################################################\n",
    "        # TODO: Your code here! \n",
    "        # Implement your aggregate function here.\n",
    "        # See here as how to use torch_scatter.scatter: \n",
    "        # https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html#torch_scatter.scatter\n",
    "        #\n",
    "        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n",
    "\n",
    "        out = torch_scatter.scatter(inputs, index, node_dim, dim_size=dim_size, reduce='mean')\n",
    "        ############################################################################\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2346aa6e-455e-438d-978a-ae5eff95f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
    "from torch_geometric.data import Data\n",
    "import scipy.sparse\n",
    "from torch import Tensor\n",
    "from torch.utils.dlpack import from_dlpack, to_dlpack\n",
    "\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be4d8051-7092-4ef9-bc01-edb02aab162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_edges=pd.read_csv('/Users/akshaj.g/Desktop/ml/social network /FacebookRecruiting/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b344fc15-bdef-46d2-86fc-45d606f4ffb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1862220\n"
     ]
    }
   ],
   "source": [
    "pos_edges = torch.tensor(pos_edges.values).T\n",
    "num_nodes = pos_edges.max().item()\n",
    "print(num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b057936-5dbd-4112-bdb1-ac2b550f4813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_scipy_sparse_matrix(\n",
    "    edge_index: Tensor,\n",
    "    edge_attr: Optional[Tensor] = None,\n",
    "    num_nodes: Optional[int] = None,\n",
    ") -> scipy.sparse.coo_matrix:\n",
    "    row, col = edge_index.cpu()\n",
    "\n",
    "    if edge_attr is None:\n",
    "        edge_attr = torch.ones(row.size(0))\n",
    "    else:\n",
    "        edge_attr = edge_attr.view(-1).cpu()\n",
    "        assert edge_attr.size(0) == row.size(0)\n",
    "\n",
    "    N = maybe_num_nodes(edge_index, num_nodes)\n",
    "    out = scipy.sparse.coo_matrix(\n",
    "        (edge_attr.numpy(), (row.numpy(), col.numpy())), (N, N))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b65fd117-91f3-47d1-88da-d1a8949133ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat=to_scipy_sparse_matrix(pos_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1066b9d8-c03e-4165-9789-f2f193b07b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import negative_sampling\n",
    "x = torch.rand(num_nodes+1, 16)\n",
    "neg_edges=negative_sampling(pos_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "706e56cc-882c-430e-a2a3-a2c7d9476c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "FB = Data(x=x, edge_index=pos_edges)\n",
    "FB.neg_edges=neg_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1ad296d-a987-4627-a198-edeaed875237",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = FB.x.shape[0]\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "FB.train_mask = train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f53e39d4-c3f6-4038-8725-3b0ef729877d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1862221, 16], edge_index=[2, 9437519], neg_edges=[2, 9437519], train_mask=[1862221])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24557935-f973-442d-b89a-a8cf98dd2bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e265a11a-13e1-49dc-98a6-2b2dc3d1b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train(dataset, args,batch_size):\n",
    "\n",
    "    print(\"Node feature task\")\n",
    "    print()\n",
    "    test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # build model\n",
    "    model = GNNStack(dataset.num_node_features, args.hidden_dim,2, \n",
    "                            args)\n",
    "    scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        model.train()\n",
    "        pos_loader = DataLoader(pos_edges, batch_size=batch_size, shuffle=True)\n",
    "        neg_loader = DataLoader(neg_edges, batch_size=batch_size, shuffle=True)\n",
    "        emb = model(dataset)\n",
    "        total_loss = 0 \n",
    "        for pos_batch, neg_batch in zip(pos_loader, neg_loader):\n",
    "            pos_batch = pos_batch[train_mask[pos_batch[:, 0]] & train_mask[pos_batch[:, 1]]]\n",
    "            neg_batch = neg_batch[train_mask[neg_batch[:, 0]] & train_mask[neg_batch[:, 1]]]\n",
    "            if len(pos_batch) == 0 or len(neg_batch) == 0:\n",
    "                continue\n",
    "            \n",
    "            pos_dot_product = torch.bmm(emb[pos_batch[:, 0]].unsqueeze(2), emb[pos_batch[:, 1]].unsqueeze(2)).squeeze()\n",
    "            neg_dot_product = torch.bmm(emb[neg_batch[:, 0]].unsqueeze(2), emb[neg_batch[:, 1]].unsqueeze(2)).squeeze()\n",
    "            pos_edge_scores = torch.sigmoid(pos_dot_product)\n",
    "            neg_edge_scores = torch.sigmoid(neg_dot_product)\n",
    "            pos_labels = torch.ones(pos_edge_scores.shape)  # Label 1 for positive edges\n",
    "            neg_labels = torch.zeros(neg_edge_scores.shape)  # Label 0 for negative edges\n",
    "\n",
    "            pos_loss = model.loss(pos_edge_scores, pos_labels)\n",
    "            neg_loss = model.loss(neg_edge_scores, neg_labels)\n",
    "            loss = pos_loss + neg_loss\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        losses.append(total_loss)\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {total_loss}, Embeddings: {emb}\")\n",
    "        \n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb6bab9b-a4f4-4a25-b73c-affeedd84705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectview:\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51c4a9d1-14d4-495e-9c39-4488a7b84214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akshaj.g/mamba/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feature task\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                             | 1/500 [00:03<29:52,  3.59s/Epochs]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0, Embeddings: tensor([[-0.5936, -0.8037],\n",
      "        [-0.4308, -1.0497],\n",
      "        [-0.6197, -0.7724],\n",
      "        ...,\n",
      "        [-0.6466, -0.7420],\n",
      "        [-0.6126, -0.7808],\n",
      "        [-0.5972, -0.7992]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                             | 2/500 [00:07<33:46,  4.07s/Epochs]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0, Embeddings: tensor([[-0.6218, -0.7700],\n",
      "        [-0.5777, -0.8237],\n",
      "        [-0.6129, -0.7804],\n",
      "        ...,\n",
      "        [-0.6163, -0.7764],\n",
      "        [-0.5268, -0.8929],\n",
      "        [-0.7247, -0.6626]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏                            | 3/500 [00:11<32:42,  3.95s/Epochs]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 0, Embeddings: tensor([[-0.7453, -0.6436],\n",
      "        [-0.5740, -0.8284],\n",
      "        [-0.5859, -0.8133],\n",
      "        ...,\n",
      "        [-0.5763, -0.8255],\n",
      "        [-0.6336, -0.7564],\n",
      "        [-0.6123, -0.7811]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏                            | 4/500 [00:15<31:47,  3.85s/Epochs]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss: 0, Embeddings: tensor([[-0.5576, -0.8500],\n",
      "        [-0.5619, -0.8443],\n",
      "        [-0.5759, -0.8260],\n",
      "        ...,\n",
      "        [-0.5130, -0.9131],\n",
      "        [-0.6450, -0.7437],\n",
      "        [-0.5891, -0.8092]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▎                            | 5/500 [00:19<32:03,  3.89s/Epochs]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss: 0, Embeddings: tensor([[-0.5207, -0.9017],\n",
      "        [-0.5150, -0.9101],\n",
      "        [-0.6506, -0.7376],\n",
      "        ...,\n",
      "        [-0.6059, -0.7887],\n",
      "        [-0.5156, -0.9092],\n",
      "        [-0.5733, -0.8293]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▎                            | 5/500 [00:22<37:02,  4.49s/Epochs]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m NODE_EMB \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, args, batch_size)\u001b[0m\n\u001b[1;32m     38\u001b[0m pos_loader \u001b[38;5;241m=\u001b[39m DataLoader(pos_edges, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     39\u001b[0m neg_loader \u001b[38;5;241m=\u001b[39m DataLoader(neg_edges, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 40\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pos_batch, neg_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pos_loader, neg_loader):\n",
      "File \u001b[0;32m~/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 48\u001b[0m, in \u001b[0;36mGNNStack.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     46\u001b[0m x, pos_edges, neg_edges\u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index , data\u001b[38;5;241m.\u001b[39mneg_edges\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m---> 48\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpos_edges\u001b[49m\u001b[43m,\u001b[49m\u001b[43mneg_edges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     50\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/mamba/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m, in \u001b[0;36mGraphSage.forward\u001b[0;34m(self, x, edge_index, neg_edges, size)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# TODO: Your code here! \u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Implement message passing, as well as any post-processing (our update rule).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Our implementation is ~5 lines, but don't worry if you deviate from this.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(pos_edges, x\u001b[38;5;241m=\u001b[39m(x, x), size\u001b[38;5;241m=\u001b[39msize)\n\u001b[0;32m---> 51\u001b[0m neg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneg_edges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_l(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_r(pos) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_r(neg)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize:\n",
      "File \u001b[0;32m~/mamba/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:484\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m         aggr_kwargs \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m res\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maggr_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_forward_hooks\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    487\u001b[0m     res \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, (aggr_kwargs, ), out)\n",
      "Cell \u001b[0;32mIn[4], line 90\u001b[0m, in \u001b[0;36mGraphSage.aggregate\u001b[0;34m(self, inputs, index, dim_size)\u001b[0m\n\u001b[1;32m     80\u001b[0m node_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# TODO: Your code here! \u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Implement your aggregate function here.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Our implementation is ~1 lines, but don't worry if you deviate from this.\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_scatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/mamba/lib/python3.10/site-packages/torch_scatter/scatter.py:156\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, out, dim_size, reduce)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scatter_mul(src, index, dim, out, dim_size)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scatter_min(src, index, dim, out, dim_size)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/mamba/lib/python3.10/site-packages/torch_scatter/scatter.py:41\u001b[0m, in \u001b[0;36mscatter_mean\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscatter_mean\u001b[39m(src: torch\u001b[38;5;241m.\u001b[39mTensor, index: torch\u001b[38;5;241m.\u001b[39mTensor, dim: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     39\u001b[0m                  out: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m                  dim_size: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 41\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mscatter_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39msize(dim)\n\u001b[1;32m     44\u001b[0m     index_dim \u001b[38;5;241m=\u001b[39m dim\n",
      "File \u001b[0;32m~/mamba/lib/python3.10/site-packages/torch_scatter/scatter.py:21\u001b[0m, in \u001b[0;36mscatter_sum\u001b[0;34m(src, index, dim, out, dim_size)\u001b[0m\n\u001b[1;32m     19\u001b[0m         size[dim] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(size, dtype\u001b[38;5;241m=\u001b[39msrc\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39msrc\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mscatter_add_(dim, index, src)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    for args in [\n",
    "        {'model_type': 'GraphSage', 'dataset': 'FB', 'num_layers': 2, 'heads': 1, 'batch_size': 1000, 'hidden_dim': 32, 'dropout': 0.5, 'epochs': 500, 'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, 'weight_decay': 5e-3, 'lr': 0.01},\n",
    "    ]:\n",
    "        args = objectview(args)\n",
    "        for model in ['GraphSage']:\n",
    "            args.model_type = model\n",
    "\n",
    "            if args.dataset == 'FB':\n",
    "                dataset = FB\n",
    "            else:\n",
    "                raise NotImplementedError(\"Unknown dataset\")\n",
    "            NODE_EMB = train(dataset, args,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee8da36-4691-4b17-974b-d70cc43fcf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train(dataset, args):\n",
    "\n",
    "    print(\"Node feature task\")\n",
    "    print()\n",
    "    test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # build model\n",
    "    model = GNNStack(dataset.num_node_features, args.hidden_dim,2, \n",
    "                            args)\n",
    "    scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch)\n",
    "            label = batch.y\n",
    "            pred = pred[batch.train_mask]\n",
    "            label = label[batch.train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "          test_acc = test(test_loader, model)\n",
    "          test_accs.append(test_acc)\n",
    "          if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "          test_accs.append(test_accs[-1])\n",
    "    \n",
    "    return test_accs, losses, best_model, best_acc, test_loader'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d567f5e-267b-4e31-9649-1b4d75a114f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
