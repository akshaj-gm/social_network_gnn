{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528d806a-a8e7-4c23-9c54-8a8013406802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://pytorch-geometric.com/whl/torch-2.3.1%2Bcu118.html\n",
      "Requirement already satisfied: torch-scatter in /Users/akshaj.g/mamba/lib/python3.10/site-packages (2.1.2)\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-2.3.1%2Bcu118.html\n",
      "Requirement already satisfied: torch-sparse in /Users/akshaj.g/mamba/lib/python3.10/site-packages (0.6.18)\n",
      "Requirement already satisfied: scipy in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-sparse) (1.10.0)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from scipy->torch-sparse) (1.24.1)\n",
      "Requirement already satisfied: torch-geometric in /Users/akshaj.g/mamba/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: tqdm in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-geometric) (4.64.1)\n",
      "Requirement already satisfied: numpy in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-geometric) (1.24.1)\n",
      "Requirement already satisfied: scipy in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-geometric) (1.10.0)\n",
      "Requirement already satisfied: jinja2 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\n",
      "Requirement already satisfied: requests in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-geometric) (2.28.1)\n",
      "Requirement already satisfied: pyparsing in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-geometric) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-geometric) (1.2.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from torch-geometric) (5.9.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from requests->torch-geometric) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from requests->torch-geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from requests->torch-geometric) (2022.12.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/akshaj.g/mamba/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Install torch geometric\n",
    "import os\n",
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "  !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-2.3.1%2Bcu118.html\n",
    "  !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-2.3.1%2Bcu118.html\n",
    "  !pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbac6c36-7836-4075-9d3e-443c71863c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_geometric\n",
    "torch_geometric.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004903ff-2e54-46c8-b5f5-14c05b5de100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Union, Tuple, Optional\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\n",
    "                                    OptTensor)\n",
    "\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "\n",
    "class GNNStack(torch.nn.Module):                                                   \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):        \n",
    "        super(GNNStack, self).__init__()                                            \n",
    "        conv_model = self.build_conv_model(args.model_type)                             \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
    "        assert (args.num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(args.num_layers-1):\n",
    "            self.convs.append(conv_model(args.heads * hidden_dim, hidden_dim))\n",
    "\n",
    "\n",
    "        self.post_mp = nn.Sequential(                                                 \n",
    "            nn.Linear(args.heads * hidden_dim, hidden_dim), nn.Dropout(args.dropout),\n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.dropout = args.dropout\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        self.emb = emb\n",
    "\n",
    "    def build_conv_model(self, model_type):\n",
    "        if model_type == 'GraphSage':\n",
    "            return GraphSage\n",
    "        elif model_type == 'GAT':\n",
    "            return GAT\n",
    "        \n",
    "    def forward(self,data):\n",
    "        #torch.autograd.set_detect_anomaly(True)\n",
    "        x, pos_edges, neg_edges= data.x, data.edge_index , data.neg_edges\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x,pos_edges,neg_edges)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout,training=self.training)\n",
    "            \n",
    "        x=self.post_mp(x)\n",
    "        \n",
    "        if self.emb==True:\n",
    "            return x\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def get_node_embeddings(self, x):\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=False)  \n",
    "        return x\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        return F.binary_cross_entropy_with_logits(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "108afeb8-2e5c-4c12-8600-335af0a5c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSage(MessagePassing):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, normalize = True,\n",
    "                 bias = False, **kwargs):  \n",
    "        super(GraphSage, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.lin_l = None\n",
    "        self.lin_r = None\n",
    "\n",
    "        self.lin_l = nn.Linear(self.in_channels, self.out_channels, bias=True)\n",
    "        self.lin_r = nn.Linear(self.in_channels, self.out_channels, bias=True)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "\n",
    "    def forward(self, x,edge_index,neg_edges,size = None):\n",
    "        #torch.autograd.set_detect_anomaly(True)\n",
    "        out = None\n",
    "        pos=self.propagate(edge_index, x=(x, x), size=size)\n",
    "        neg=self.propagate(neg_edges, x=(x, x), size=size)\n",
    "        out=self.lin_l(x) + self.lin_r(pos) - self.lin_r(neg)\n",
    "        if self.normalize:\n",
    "            out = F.normalize(out, p=2)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):\n",
    "        out = None\n",
    "        out = x_j\n",
    "        return out\n",
    "    \n",
    "    def aggregate(self, inputs, index, dim_size = None):\n",
    "        out = None\n",
    "        node_dim = self.node_dim\n",
    "        out = torch_scatter.scatter(inputs, index, node_dim, dim_size=dim_size, reduce='mean')\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2346aa6e-455e-438d-978a-ae5eff95f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n",
    "from torch_geometric.data import Data\n",
    "import scipy.sparse\n",
    "from torch import Tensor\n",
    "from torch.utils.dlpack import from_dlpack, to_dlpack\n",
    "\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be4d8051-7092-4ef9-bc01-edb02aab162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_edges=pd.read_csv('/Users/akshaj.g/Desktop/ml/social network /FacebookRecruiting/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b344fc15-bdef-46d2-86fc-45d606f4ffb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1862220\n"
     ]
    }
   ],
   "source": [
    "pos_edges = torch.tensor(pos_edges.values).T\n",
    "num_nodes = pos_edges.max().item()\n",
    "print(num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b057936-5dbd-4112-bdb1-ac2b550f4813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_scipy_sparse_matrix(\n",
    "    edge_index: Tensor,\n",
    "    edge_attr: Optional[Tensor] = None,\n",
    "    num_nodes: Optional[int] = None,\n",
    ") -> scipy.sparse.coo_matrix:\n",
    "    row, col = edge_index.cpu()\n",
    "\n",
    "    if edge_attr is None:\n",
    "        edge_attr = torch.ones(row.size(0))\n",
    "    else:\n",
    "        edge_attr = edge_attr.view(-1).cpu()\n",
    "        assert edge_attr.size(0) == row.size(0)\n",
    "\n",
    "    N = maybe_num_nodes(edge_index, num_nodes)\n",
    "    out = scipy.sparse.coo_matrix(\n",
    "        (edge_attr.numpy(), (row.numpy(), col.numpy())), (N, N))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b65fd117-91f3-47d1-88da-d1a8949133ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_mat=to_scipy_sparse_matrix(pos_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1066b9d8-c03e-4165-9789-f2f193b07b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import negative_sampling\n",
    "x = torch.rand(num_nodes+1, 16)\n",
    "neg_edges=negative_sampling(pos_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "706e56cc-882c-430e-a2a3-a2c7d9476c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "FB = Data(x=x, edge_index=pos_edges)\n",
    "FB.neg_edges=neg_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1ad296d-a987-4627-a198-edeaed875237",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = FB.x.shape[0]\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "FB.train_mask = train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f53e39d4-c3f6-4038-8725-3b0ef729877d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1862221, 16], edge_index=[2, 9437519], neg_edges=[2, 9437519], train_mask=[1862221])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24557935-f973-442d-b89a-a8cf98dd2bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e265a11a-13e1-49dc-98a6-2b2dc3d1b6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import time\\n\\nimport networkx as nx\\nimport numpy as np\\nimport torch\\nimport torch.optim as optim\\nfrom tqdm import trange\\nimport pandas as pd\\nimport copy\\n\\nfrom torch_geometric.datasets import TUDataset\\nfrom torch_geometric.datasets import Planetoid\\nfrom torch_geometric.data import DataLoader\\n\\nimport torch_geometric.nn as pyg_nn\\n\\nimport matplotlib.pyplot as plt\\n\\n\\ndef train(dataset, args,batch_size):\\n\\n    print(\"Node feature task\")\\n    print()\\n    #test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\\n    # build model\\n    model = GNNStack(dataset.num_node_features, args.hidden_dim,2, \\n                            args)\\n    scheduler, opt = build_optimizer(args, model.parameters())\\n\\n    # train\\n    losses = []\\n    test_accs = []\\n    best_acc = 0\\n    best_model = None\\n    x_values = []\\n    y_values = []\\n    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\\n        print(\"Entering the 1 loop\")\\n        model.train()\\n        #pos_loader = DataLoader(dataset.pos_edges, batch_size=batch_size, shuffle=True)\\n        #neg_loader = DataLoader(dataset.neg_edges, batch_size=batch_size, shuffle=True)\\n        pos_loader = dataset.edge_index\\n        neg_loader = dataset.neg_edges\\n        emb = model(dataset)\\n        total_loss = 0\\n        opt.zero_grad()\\n        for pos_batch, neg_batch in zip(pos_loader, neg_loader):\\n            pos_batch = pos_batch[train_mask[pos_batch[:, 0]] & train_mask[pos_batch[:, 1]]]\\n            neg_batch = neg_batch[train_mask[neg_batch[:, 0]] & train_mask[neg_batch[:, 1]]]\\n            print(len(pos_batch))\\n            if len(pos_batch) == 0 or len(neg_batch) == 0:\\n                print(\"Skipping the loop due to empty batches\")\\n                continue\\n            print(\"Entering the 2 loop\")\\n            pos_dot_product = torch.bmm(emb[pos_batch[:, 0]].unsqueeze(2), emb[pos_batch[:, 1]].unsqueeze(2)).squeeze()\\n            neg_dot_product = torch.bmm(emb[neg_batch[:, 0]].unsqueeze(2), emb[neg_batch[:, 1]].unsqueeze(2)).squeeze()\\n            print(\"pos_dot_product:\", pos_dot_product)\\n            print(\"neg_dot_product:\", neg_dot_product)\\n            pos_edge_scores = pos_dot_product\\n            neg_edge_scores = neg_dot_product\\n            print(\"pos_edge_scores:\", pos_edge_scores)\\n            print(\"neg_edge_scores:\", neg_edge_scores)\\n            pos_labels = torch.ones(pos_edge_scores.shape)  # Label 1 for positive edges\\n            neg_labels = torch.zeros(neg_edge_scores.shape)  # Label 0 for negative edges\\n            print(\"pos_labels:\", pos_labels)\\n            print(\"neg_labels:\", neg_labels)\\n            pos_loss = model.loss(pos_edge_scores, pos_labels)\\n            neg_loss = model.loss(neg_edge_scores, neg_labels)\\n            print(\"pos_loss:\", pos_loss.item())\\n            print(\"neg_loss:\", neg_loss.item())\\n            loss = pos_loss + neg_loss\\n            loss.backward()\\n            opt.step()\\n            total_loss += loss.item()\\n        \\n        losses.append(total_loss)\\n        x_values.append(emb[0][0].item())\\n        y_values.append(emb[690568][0].item())\\n        print(f\"Epoch: {epoch+1}, Loss: {total_loss}, Embeddings: {emb}\")\\n\\n    plt.figure(figsize=(8, 6))\\n    \\n    # Ensure x_values and y_values have the same length\\n    min_length = min(len(x_values), len(y_values))\\n    \\n    plt.scatter(range(1, min_length + 1), x_values[:min_length], label=\\'X Value\\')\\n    plt.scatter(range(1, min_length + 1), y_values[:min_length], label=\\'Y Value\\')\\n    plt.title(\\'Embedding Values for the First Node across Epochs\\')\\n    plt.xlabel(\\'Epochs\\')\\n    plt.ylabel(\\'Embedding Values\\')\\n    plt.legend()\\n    plt.show()\\n        \\n    return emb'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train(dataset, args,batch_size):\n",
    "\n",
    "    print(\"Node feature task\")\n",
    "    print()\n",
    "    #test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    # build model\n",
    "    model = GNNStack(dataset.num_node_features, args.hidden_dim,2, \n",
    "                            args)\n",
    "    scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        print(\"Entering the 1 loop\")\n",
    "        model.train()\n",
    "        #pos_loader = DataLoader(dataset.pos_edges, batch_size=batch_size, shuffle=True)\n",
    "        #neg_loader = DataLoader(dataset.neg_edges, batch_size=batch_size, shuffle=True)\n",
    "        pos_loader = dataset.edge_index\n",
    "        neg_loader = dataset.neg_edges\n",
    "        emb = model(dataset)\n",
    "        total_loss = 0\n",
    "        opt.zero_grad()\n",
    "        for pos_batch, neg_batch in zip(pos_loader, neg_loader):\n",
    "            pos_batch = pos_batch[train_mask[pos_batch[:, 0]] & train_mask[pos_batch[:, 1]]]\n",
    "            neg_batch = neg_batch[train_mask[neg_batch[:, 0]] & train_mask[neg_batch[:, 1]]]\n",
    "            print(len(pos_batch))\n",
    "            if len(pos_batch) == 0 or len(neg_batch) == 0:\n",
    "                print(\"Skipping the loop due to empty batches\")\n",
    "                continue\n",
    "            print(\"Entering the 2 loop\")\n",
    "            pos_dot_product = torch.bmm(emb[pos_batch[:, 0]].unsqueeze(2), emb[pos_batch[:, 1]].unsqueeze(2)).squeeze()\n",
    "            neg_dot_product = torch.bmm(emb[neg_batch[:, 0]].unsqueeze(2), emb[neg_batch[:, 1]].unsqueeze(2)).squeeze()\n",
    "            print(\"pos_dot_product:\", pos_dot_product)\n",
    "            print(\"neg_dot_product:\", neg_dot_product)\n",
    "            pos_edge_scores = pos_dot_product\n",
    "            neg_edge_scores = neg_dot_product\n",
    "            print(\"pos_edge_scores:\", pos_edge_scores)\n",
    "            print(\"neg_edge_scores:\", neg_edge_scores)\n",
    "            pos_labels = torch.ones(pos_edge_scores.shape)  # Label 1 for positive edges\n",
    "            neg_labels = torch.zeros(neg_edge_scores.shape)  # Label 0 for negative edges\n",
    "            print(\"pos_labels:\", pos_labels)\n",
    "            print(\"neg_labels:\", neg_labels)\n",
    "            pos_loss = model.loss(pos_edge_scores, pos_labels)\n",
    "            neg_loss = model.loss(neg_edge_scores, neg_labels)\n",
    "            print(\"pos_loss:\", pos_loss.item())\n",
    "            print(\"neg_loss:\", neg_loss.item())\n",
    "            loss = pos_loss + neg_loss\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        losses.append(total_loss)\n",
    "        x_values.append(emb[0][0].item())\n",
    "        y_values.append(emb[690568][0].item())\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {total_loss}, Embeddings: {emb}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Ensure x_values and y_values have the same length\n",
    "    min_length = min(len(x_values), len(y_values))\n",
    "    \n",
    "    plt.scatter(range(1, min_length + 1), x_values[:min_length], label='X Value')\n",
    "    plt.scatter(range(1, min_length + 1), y_values[:min_length], label='Y Value')\n",
    "    plt.title('Embedding Values for the First Node across Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Embedding Values')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "    return emb'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80c88d40-30b3-4355-8491-c10c8f9f2ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import traceback\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "def train(dataset, args, batch_size):\n",
    "    print(\"Node feature task\")\n",
    "    print()\n",
    "    \n",
    "    # Build model\n",
    "    model = GNNStack(dataset.num_node_features, args.hidden_dim, 4, args)\n",
    "    scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # Train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "\n",
    "    pos_edges = dataset.edge_index.t()\n",
    "    neg_edges = dataset.neg_edges.t()\n",
    "    print(pos_edges.shape)\n",
    "\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        print(\"Entering the 1 loop\")\n",
    "        model.train()\n",
    "        emb = model(dataset)\n",
    "        total_loss = 0\n",
    "        opt.zero_grad()\n",
    "\n",
    "        pos_edges_shuffle_indices = torch.randperm(pos_edges.size(0))\n",
    "        pos_edges_shuffled = pos_edges[pos_edges_shuffle_indices]\n",
    "        pos_edges_shuffled = pos_edges_shuffled[:len(pos_edges_shuffled)//batch_size * batch_size].view(-1, batch_size, 2)\n",
    "\n",
    "        neg_edges_shuffle_indices = torch.randperm(neg_edges.size(0))\n",
    "        neg_edges_shuffled = neg_edges[neg_edges_shuffle_indices]\n",
    "        neg_edges_shuffled = neg_edges_shuffled[:len(neg_edges_shuffled)//batch_size * batch_size].view(-1, batch_size, 2)\n",
    "\n",
    "        print(pos_edges_shuffled.shape)\n",
    "        for pos_batch, neg_batch in zip(pos_edges_shuffled, neg_edges_shuffled):\n",
    "            pos_batch_flat = pos_batch.view(-1)\n",
    "            neg_batch_flat = neg_batch.view(-1)\n",
    "            with torch.no_grad():\n",
    "                pos_batch_flat = torch.where(train_mask[pos_batch_flat], torch.tensor(1), pos_batch_flat.clone())\n",
    "                neg_batch_flat = torch.where(train_mask[neg_batch_flat], torch.tensor(1), neg_batch_flat.clone())\n",
    "\n",
    "\n",
    "            \n",
    "            print(pos_batch_flat.shape)\n",
    "            pos_batch = pos_batch_flat.view(pos_batch.shape)\n",
    "            neg_batch = neg_batch_flat.view(neg_batch.shape)\n",
    "            \n",
    "            print(pos_batch.shape)\n",
    "            # Filter out edges based on train_mask\n",
    "            #pos_batch_filtered = pos_batch[train_mask[pos_batch[:, 0]] & train_mask[pos_batch[:, 1]]]\n",
    "            #neg_batch_filtered = neg_batch[train_mask[neg_batch[:, 0]] & train_mask[neg_batch[:, 1]]]\n",
    "\n",
    "\n",
    "            if len(pos_batch) == 0 or len(neg_batch) == 0:\n",
    "                print(\"Skipping the loop due to empty batches\")\n",
    "                continue\n",
    "\n",
    "            print(\"Entering the 2 loop\")\n",
    "         \n",
    "            pos_dot_product = torch.bmm(emb[pos_batch[:, 0].unsqueeze(1)], emb[pos_batch[:, 1].unsqueeze(1)].transpose(1, 2)).squeeze()\n",
    "            neg_dot_product = torch.bmm(emb[neg_batch[:, 0].unsqueeze(1)], emb[neg_batch[:, 1].unsqueeze(1)].transpose(1, 2)).squeeze()\n",
    "            pos_edge_scores = pos_dot_product\n",
    "            neg_edge_scores = neg_dot_product\n",
    "            pos_labels = torch.ones(pos_edge_scores.shape)  \n",
    "            neg_labels = torch.zeros(neg_edge_scores.shape)  \n",
    "            pos_loss = model.loss(pos_edge_scores, pos_labels)\n",
    "            neg_loss = model.loss(neg_edge_scores, neg_labels)\n",
    "            print(\"pos_loss:\", pos_loss.item())\n",
    "            print(\"neg_loss:\", neg_loss.item())\n",
    "            \n",
    "            import pdb; pdb.set_trace()\n",
    "            \n",
    "            loss = pos_loss + neg_loss\n",
    "            loss.backward(retain_graph=True)\n",
    "            opt.step()\n",
    "            total_loss += loss.detach().item()\n",
    "        \n",
    "        losses.append(total_loss)\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {total_loss}, Embeddings: {emb}\")\n",
    "\n",
    "\n",
    "        \n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb6bab9b-a4f4-4a25-b73c-affeedd84705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectview:\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51c4a9d1-14d4-495e-9c39-4488a7b84214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feature task\n",
      "\n",
      "torch.Size([9437519, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                      | 0/50 [00:00<?, ?Epochs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering the 1 loop\n",
      "torch.Size([9437, 1000, 2])\n",
      "torch.Size([2000])\n",
      "torch.Size([1000, 2])\n",
      "Entering the 2 loop\n",
      "pos_loss: 0.00040967160020954907\n",
      "neg_loss: 7.808108806610107\n",
      "> \u001b[0;32m/var/folders/_2/2n3n9_3x6ml82z7y8mx8sgbm0000gn/T/ipykernel_63102/3147216581.py\u001b[0m(95)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     93 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     94 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 95 \u001b[0;31m            \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mneg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     96 \u001b[0;31m            \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     97 \u001b[0;31m            \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/var/folders/_2/2n3n9_3x6ml82z7y8mx8sgbm0000gn/T/ipykernel_63102/3147216581.py\u001b[0m(96)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     94 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     95 \u001b[0;31m            \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mneg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 96 \u001b[0;31m            \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     97 \u001b[0;31m            \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     98 \u001b[0;31m            \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/var/folders/_2/2n3n9_3x6ml82z7y8mx8sgbm0000gn/T/ipykernel_63102/3147216581.py\u001b[0m(97)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     95 \u001b[0;31m            \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mneg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     96 \u001b[0;31m            \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 97 \u001b[0;31m            \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     98 \u001b[0;31m            \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     99 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000])\n",
      "torch.Size([1000, 2])\n",
      "Entering the 2 loop\n",
      "pos_loss: 0.0004099216894246638\n",
      "neg_loss: 7.809834957122803\n",
      "> \u001b[0;32m/var/folders/_2/2n3n9_3x6ml82z7y8mx8sgbm0000gn/T/ipykernel_63102/3147216581.py\u001b[0m(95)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     93 \u001b[0;31m            \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     94 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 95 \u001b[0;31m            \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mneg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     96 \u001b[0;31m            \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     97 \u001b[0;31m            \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/var/folders/_2/2n3n9_3x6ml82z7y8mx8sgbm0000gn/T/ipykernel_63102/3147216581.py\u001b[0m(96)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     94 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     95 \u001b[0;31m            \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mneg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 96 \u001b[0;31m            \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     97 \u001b[0;31m            \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     98 \u001b[0;31m            \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [16, 4]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n",
      "> \u001b[0;32m/var/folders/_2/2n3n9_3x6ml82z7y8mx8sgbm0000gn/T/ipykernel_63102/3147216581.py\u001b[0m(96)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     94 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     95 \u001b[0;31m            \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mneg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 96 \u001b[0;31m            \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     97 \u001b[0;31m            \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     98 \u001b[0;31m            \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                      | 0/50 [00:49<?, ?Epochs/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m/var/folders/_2/2n3n9_3x6ml82z7y8mx8sgbm0000gn/T/ipykernel_63102/3147216581.py\u001b[0m(96)\u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     94 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     95 \u001b[0;31m            \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mneg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 96 \u001b[0;31m            \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     97 \u001b[0;31m            \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     98 \u001b[0;31m            \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [16, 4]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n",
      "> \u001b[0;32m/var/folders/_2/2n3n9_3x6ml82z7y8mx8sgbm0000gn/T/ipykernel_63102/2969017040.py\u001b[0m(13)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      9 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'FB'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     10 \u001b[0;31m                \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     11 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m                \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0mNODE_EMB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m/var/folders/_2/2n3n9_3x6ml82z7y8mx8sgbm0000gn/T/ipykernel_63102/2969017040.py\u001b[0m(13)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      9 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'FB'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     10 \u001b[0;31m                \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     11 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     12 \u001b[0;31m                \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 13 \u001b[0;31m            \u001b[0mNODE_EMB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m    [... skipped 1 hidden frame]\u001b[0m\n",
      "\n",
      "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [16, 4]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n",
      "> \u001b[0;32m/Users/akshaj.g/mamba/lib/python3.10/site-packages/IPython/core/interactiveshell.py\u001b[0m(3442)\u001b[0;36mrun_code\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   3440 \u001b[0;31m                    \u001b[0;32mawait\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   3441 \u001b[0;31m                \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 3442 \u001b[0;31m                    \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   3443 \u001b[0;31m            \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   3444 \u001b[0;31m                \u001b[0;31m# Reset our crash handler in place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m    [... skipped 1 hidden frame]\u001b[0m\n",
      "\n",
      "> \u001b[0;32m/Users/akshaj.g/mamba/lib/python3.10/site-packages/IPython/core/interactiveshell.py\u001b[0m(3445)\u001b[0;36mrun_code\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   3443 \u001b[0;31m            \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   3444 \u001b[0;31m                \u001b[0;31m# Reset our crash handler in place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 3445 \u001b[0;31m                \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexcepthook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_excepthook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   3446 \u001b[0;31m        \u001b[0;32mexcept\u001b[0m \u001b[0mSystemExit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   3447 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [16, 4]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m NODE_EMB \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 96\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, args, batch_size)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdb\u001b[39;00m; pdb\u001b[38;5;241m.\u001b[39mset_trace()\n\u001b[1;32m     95\u001b[0m loss \u001b[38;5;241m=\u001b[39m pos_loss \u001b[38;5;241m+\u001b[39m neg_loss\n\u001b[0;32m---> 96\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     98\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/mamba/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mamba/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [16, 4]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
    "    for args in [\n",
    "        {'model_type': 'GraphSage', 'dataset': 'FB', 'num_layers': 2, 'heads': 1, 'batch_size': 1000, 'hidden_dim': 16, 'dropout': 0.5, 'epochs': 50, 'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, 'weight_decay': 5e-3, 'lr': 0.001},\n",
    "    ]:\n",
    "        args = objectview(args)\n",
    "        for model in ['GraphSage']:\n",
    "            args.model_type = model\n",
    "\n",
    "            if args.dataset == 'FB':\n",
    "                dataset = FB\n",
    "            else:\n",
    "                raise NotImplementedError(\"Unknown dataset\")\n",
    "            NODE_EMB = train(dataset, args,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee8da36-4691-4b17-974b-d70cc43fcf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train(dataset, args):\n",
    "\n",
    "    print(\"Node feature task\")\n",
    "    print()\n",
    "    test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # build model\n",
    "    model = GNNStack(dataset.num_node_features, args.hidden_dim,2, \n",
    "                            args)\n",
    "    scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch)\n",
    "            label = batch.y\n",
    "            pred = pred[batch.train_mask]\n",
    "            label = label[batch.train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "          test_acc = test(test_loader, model)\n",
    "          test_accs.append(test_acc)\n",
    "          if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "          test_accs.append(test_accs[-1])\n",
    "    \n",
    "    return test_accs, losses, best_model, best_acc, test_loader'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d567f5e-267b-4e31-9649-1b4d75a114f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
